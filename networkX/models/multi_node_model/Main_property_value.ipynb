{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce6cc43-7731-4296-993b-b03036a750a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Icarusight.queries.get_queries import query_canape_product_df, query_product_properties_df\n",
    "from Icarusight.Icarusight.utils import *\n",
    "from Icarusight.Icarusight.property_value_model.property_value_model import create_property_value_graph\n",
    "from Icarusight.Icarusight.clustering import get_cluster_labels, apply_clustering_algorithm, remove_meaningless_clusters\n",
    "from Icarusight.Icarusight.vectorizer.custom_tfidf_transformer_class import ClassTfidfTransformer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from cdiscount import snowflake, config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b0f554-8b57-46d6-8786-03a64cf7d59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a97b16d-5145-4c0c-9642-d3b9d172ffd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"Icarusight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed403fe5-dc20-4bc6-84d5-f2b8233eff14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting queries... Done, length of product_df is: 102877\n"
     ]
    }
   ],
   "source": [
    "secrets = config.load_secrets('queries/secrets.yml', key='snowflake')\n",
    "con = snowflake.get_snowflake_connection(**secrets)\n",
    "print(\"Starting queries.\", end=\"\")\n",
    "product_df = query_canape_product_df(con)\n",
    "print(\".\", end=\"\")\n",
    "product_property_df = query_product_properties_df(con)\n",
    "print(\". Done, length of product_df is:\", len(product_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0739d0af-382a-4cf8-8c92-afaaa1ff6838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(refined_str(product_property_df['product_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523a7acb-1952-47e5-b383-d3ad8b3d13f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def add_and_update_edges_from_list(g, input_list, property_id):\n",
    "    print(combinations(input_list, 2))\n",
    "    for product_1, product_2 in combinations(input_list, 2):\n",
    "        weight = 1\n",
    "        edge_data = g.get_edge_data(product_1, product_2)\n",
    "        if edge_data is not None:\n",
    "            weight = edge_data.get(\"weight\", 1) + 1\n",
    "        g.add_edge(product_1, product_2, weight=weight, relationship=property_id)\n",
    "\n",
    "\n",
    "def add_nodes(g, product_df):\n",
    "    for product_id in tqdm(product_df['product_id'], total=len(product_df['product_id'])):\n",
    "        g.add_node(product_id)\n",
    "\n",
    "\n",
    "def create_property_value_graph(product_df, product_property_df):\n",
    "    g = nx.Graph()\n",
    "    print('Adding nodes...')\n",
    "    add_nodes(g, product_df)\n",
    "    print('Nodes added, adding edges...')\n",
    "    for i, row in tqdm(product_property_df.iterrows(), total=len(product_property_df)):\n",
    "        add_and_update_edges_from_list(g, refined_str(row['product_ids']), row['property_id'])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2a5e15-44b2-4ada-9210-61e5474cc7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting graph creation\n",
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102877/102877 [00:00<00:00, 739212.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes added, adding edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.combinations object at 0x7fba24804e00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting graph creation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_property_value_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct_property_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph creation done.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(g\u001b[38;5;241m.\u001b[39mnodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(g\u001b[38;5;241m.\u001b[39medges)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m edges.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting clustering...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mcreate_property_value_graph\u001b[0;34m(product_df, product_property_df)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNodes added, adding edges...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(product_property_df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(product_property_df)):\n\u001b[0;32m---> 26\u001b[0m     \u001b[43madd_and_update_edges_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefined_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_property_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36madd_and_update_edges_from_list\u001b[0;34m(g, input_list, property_id)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     weight \u001b[38;5;241m=\u001b[39m edge_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelationship\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperty_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/jupyter/.virtualenvs/icarusight/lib/python3.10/site-packages/networkx/classes/graph.py:950\u001b[0m, in \u001b[0;36mGraph.add_edge\u001b[0;34m(self, u_of_edge, v_of_edge, **attr)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjlist_inner_dict_factory()\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_attr_dict_factory()\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone cannot be a node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(\"Starting graph creation\")\n",
    "g = create_property_value_graph(product_df, product_property_df)\n",
    "print(\"Graph creation done.\", f\"got {len(g.nodes)} nodes and {len(g.edges)} edges.\")\n",
    "print(\"Starting clustering...\")\n",
    "partitions = apply_clustering_algorithm(g)\n",
    "print(\"Clustering done, removing the meaningless clusters...\")\n",
    "partitions = remove_meaningless_clusters(partitions, n=3)\n",
    "print(\"Removing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f4f544-9082-4abf-98c1-468852dc4c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('product_df', 98424522),\n",
       " ('product_property_df', 4512254),\n",
       " ('ClassTfidfTransformer', 1072),\n",
       " ('Counter', 1072),\n",
       " ('Network', 1072),\n",
       " ('combinations', 408),\n",
       " ('defaultdict', 408),\n",
       " ('add_and_update_edges_from_list', 144),\n",
       " ('add_nodes', 144),\n",
       " ('apply_clustering_algorithm', 144),\n",
       " ('cleaning_strings', 144),\n",
       " ('create_property_value_graph', 144),\n",
       " ('df2features', 144),\n",
       " ('get_cluster_labels', 144),\n",
       " ('open', 144),\n",
       " ('query_canape_product_df', 144),\n",
       " ('query_product_properties_df', 144),\n",
       " ('refined_str', 144),\n",
       " ('remove_meaningless_clusters', 144),\n",
       " ('str2value_mapping', 144),\n",
       " ('write_html_from_nx', 144),\n",
       " ('config', 72),\n",
       " ('np', 72),\n",
       " ('nx', 72),\n",
       " ('pd', 72),\n",
       " ('plt', 72),\n",
       " ('brands_mapping', 64),\n",
       " ('categories_mapping', 64),\n",
       " ('colors_mapping', 64),\n",
       " ('name_mapping', 64),\n",
       " ('partitions', 64),\n",
       " ('sellers_mapping', 64),\n",
       " ('con', 48),\n",
       " ('g', 48),\n",
       " ('nlp', 48),\n",
       " ('stopwords', 48)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab784fb-131b-4f90-b918-27e03562152d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775670678.0\n"
     ]
    }
   ],
   "source": [
    "# Estimating the total number of edges.\n",
    "count = 0\n",
    "for i, row in product_property_df.iterrows():\n",
    "    n = len(refined_str(row['product_ids']))\n",
    "    count += (n*(n-1))/2\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e9021a-f924-4d53-ad1a-b5910f5db594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_results(path_to_save, cluster_infos, cluster_top_words_count, cluster_top_words_tfidf):\n",
    "    # Create a DataFrame from the dictionaries\n",
    "    cluster_labels, cluster_max_values, cluster_total_len = cluster_infos[0], cluster_infos[1], cluster_infos[2]\n",
    "    df = pd.DataFrame({\n",
    "        'cluster_id': list(cluster_labels.keys()),\n",
    "        'label': list(cluster_labels.values()),\n",
    "        'max_values': list(cluster_max_values.values()),\n",
    "        'total_len': list(cluster_total_len.values()),\n",
    "        'top_words_count_method': list(cluster_top_words_count),\n",
    "        'top_words_tfidf_method': list(cluster_top_words_tfidf)\n",
    "    })\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    df.to_csv(path_to_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed17013d-ef3b-408a-ab42-264bfd5bf79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355782/1920937987.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  product_restrained_df['name_descr'] = product_restrained_df.product_name +  \". \" + product_restrained_df.product_long_description + \". \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id\n",
      "1.0        ZHI Canapé lit à 2 places Noir Tissu 738650263...\n",
      "68.0       Canapé lit élégant Canapé moderne Intérieur mo...\n",
      "87.0       Canapé convertible 3 places avec coffre de ran...\n",
      "144.0      MOO Canapé lit à 2 places Vert foncé Velours 7...\n",
      "162.0      Canapé lit à 2 places et deux oreillers Rouge ...\n",
      "184.0      Canapé 3 places et fauteuil CHESTERFIELD Simil...\n",
      "226.0      Canapé lit STAR scandinave Sofa convertible av...\n",
      "715.0      Canapé Chesterfield en forme de L Cuir synthét...\n",
      "1058.0     Banquette lit BZ matelas HR 140 cm YASMO n 2. ...\n",
      "1357.0     Canapé d angle panoramique Dante en U en velou...\n",
      "2264.0     Beliani Chaise longue rose poudré côté droit M...\n",
      "2377.0     Canapé lit à 2 places Gris clair Velours DIOCH...\n",
      "4303.0     Micadoni Home JUSTIN Canapé d angle 4 places e...\n",
      "5138.0     Canapé lit à 2 places Marron Microfibre SALALI...\n",
      "5407.0     YaJiaSheng Ensemble de canapés à 2 et à 3 plac...\n",
      "5470.0     LVL MEUBLE SOFA Canapé lit à 2 places Rose Vel...\n",
      "5505.0     ABB Canapé lit à 2 places Marron Tissu microfi...\n",
      "7682.0     Canapé angle ALMA convertible tissu rouille 4 ...\n",
      "8078.0     Micadoni Home JADE Canapé 3 places en velours ...\n",
      "8861.0     Micadoni Home JODIE Canapé d angle 4 places en...\n",
      "9998.0     Micadoni Home MILEY Canapé d angle symétrique ...\n",
      "11293.0    Canapé lit à 2 places Crème Tissu microfibre A...\n",
      "11462.0    Micadoni Home MILEY Canapé d angle gauche 6 pl...\n",
      "11726.0    8499Magnifique Canapé droit fixe 3 places Cana...\n",
      "12407.0    Pwshymi Canapé à 3 places avec repose pied Rou...\n",
      "14047.0    Micadoni Home RUBY Canapé d angle gauche 5 pla...\n",
      "18784.0    MICADONI Canapé d angle Gauche Sovite 5 places...\n",
      "27442.0    Grand canapé Sirpio XL microfibre marron kaki ...\n",
      "34442.0    Zerodis Canapé lit à 2 places avec repose pied...\n",
      "34541.0    Canapes Ensemble de canape 2 pcs Tissu Noir. E...\n",
      "41945.0    Miliboo Canapé convertible scandinave 3 places...\n",
      "Name: name_descr, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#product_df.set_index(\"product_id\", inplace=True)\n",
    "\n",
    "product_df['cluster_id'] = partitions\n",
    "product_restrained_df = product_df[~product_df['cluster_id'].isnull()]\n",
    "product_restrained_df.astype({'cluster_id': int})\n",
    "product_restrained_df['name_descr'] = product_restrained_df.product_name +  \". \" + product_restrained_df.product_long_description + \". \"\n",
    "cluster_serie = product_restrained_df.groupby(\"cluster_id\").name_descr.sum()\n",
    "print(cluster_serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c06618-154a-45eb-9b87-ae39f10233a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_tf_idf_words(response, threshold, top_n=2):\n",
    "    #response_normalized = normalize(response, axis=1, norm='l2')\n",
    "    response_normalized = custom_norm(response)\n",
    "    response_normalized.data[response_normalized.data < threshold] = 0.0\n",
    "    response_normalized.eliminate_zeros()\n",
    "    #print(response_normalized.data)\n",
    "    sorted_nzs = np.argsort(response_normalized.data)[:-(top_n+1):-1]\n",
    "    res = feature_array[response_normalized.indices[sorted_nzs[ response_normalized.indices[sorted_nzs] > threshold]]]\n",
    "    return np.apply_along_axis(' | '.join, 0, res)\n",
    "\n",
    "def custom_norm(x):\n",
    "    norm = x.sum(axis=1)\n",
    "    return x / norm\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86add6bc-6412-49dd-8671-ac46d752c903",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# COUNT VECTORIZER METHOD\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(cluster_serie)\n\u001b[1;32m      6\u001b[0m top_words_per_cluster_count \u001b[38;5;241m=\u001b[39m [get_top_tf_idf_words(item, \u001b[38;5;241m0.015\u001b[39m, \u001b[38;5;241m5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m vector]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "# COUNT VECTORIZER METHOD\n",
    "vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('french'))\n",
    "vector = vectorizer.fit_transform(cluster_serie)\n",
    "top_words_per_cluster_count = [get_top_tf_idf_words(item, 0.015, 5) for item in vector]\n",
    "for top_words in top_words_per_cluster_count:\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04cba884-a9ca-4246-b95a-44ea46010916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomClassTfidfTransformer(ClassTfidfTransformer):\n",
    "    def __init__(self, use_idf: bool = False, bm25_weighting: bool = False, reduce_frequent_words: bool = False):\n",
    "        super(CustomClassTfidfTransformer, self).__init__(\n",
    "            bm25_weighting=bm25_weighting,\n",
    "            reduce_frequent_words=reduce_frequent_words\n",
    "        )\n",
    "        self.use_idf = use_idf\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "        if self.use_idf:\n",
    "\n",
    "            if self.reduce_frequent_words:\n",
    "                X.data = np.sqrt(X.data)\n",
    "\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd965b4c-db67-42fb-89f7-fef8450f7dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TFIDF VECTORIZER METHOD\u001b[39;00m\n\u001b[1;32m      4\u001b[0m v \u001b[38;5;241m=\u001b[39m CustomClassTfidfTransformer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m x \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mvector\u001b[49m)\n\u001b[1;32m      6\u001b[0m feature_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      7\u001b[0m tfidf_sorting \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(x\u001b[38;5;241m.\u001b[39mtoarray())\u001b[38;5;241m.\u001b[39mflatten()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TFIDF VECTORIZER METHOD\n",
    "v = CustomClassTfidfTransformer(use_idf=True)\n",
    "x = v.fit_transform(vector)\n",
    "feature_array = np.array(vectorizer.get_feature_names_out())\n",
    "tfidf_sorting = np.argsort(x.toarray()).flatten()[::-1]\n",
    "top_words_per_cluster_tfidf = [get_top_tf_idf_words(item, 0.015, 5) for item in x]\n",
    "for top_words in top_words_per_cluster_tfidf:\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5bbd2ca-6cc4-412e-be0c-bfa5ecab13dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10731/10731 [07:43<00:00, 23.16it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 302380.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "/home/jupyter/Icarusight\n",
      "Results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_infos = get_cluster_labels(g, partitions)\n",
    "print(\"Saving results...\")\n",
    "save_results(\"results/results_threshold_at_0.015_cutoff_2.csv\", cluster_infos, top_words_per_cluster_count, top_words_per_cluster_tfidf)\n",
    "print(os.getcwd())\n",
    "print(\"Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (icarusight)",
   "language": "python",
   "name": "icarusight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
