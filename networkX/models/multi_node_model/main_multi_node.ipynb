{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bf881b-9bf4-4e5b-9df1-0ff9d7797293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/jupyter/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Icarusight.queries.get_queries import query_canape_product_df, query_product_properties_df\n",
    "from Icarusight.Icarusight.utils import *\n",
    "from Icarusight.Icarusight.clustering import get_cluster_labels, apply_clustering_algorithm, remove_meaningless_clusters\n",
    "from Icarusight.Icarusight.multi_node_model.multi_node_model import add_product_node\n",
    "from cdiscount import snowflake, config\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "os.chdir(\"Icarusight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f797b93-e10c-4c31-9544-0a7f48bebac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102877/102877 [01:54<00:00, 898.06it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 16396.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes added, adding edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 82.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph creation done. got 102885 nodes and 204933 edges.\n",
      "Starting clustering...\n",
      "Clustering done, removing the meaningless clusters...\n",
      "Removing done.\n"
     ]
    }
   ],
   "source": [
    "g = create_property_value_graph(product_df, product_property_df)\n",
    "print(\"Graph creation done.\", end=\" \")\n",
    "print(f\"got {len(g.nodes)} nodes and {len(g.edges)} edges.\")\n",
    "\n",
    "print(\"Starting clustering...\")\n",
    "partitions = apply_clustering_algorithm(g)\n",
    "print(\"Clustering done, removing the meaningless clusters...\")\n",
    "partitions = remove_meaningless_clusters(partitions, n=3)\n",
    "print(\"Removing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b44a6436-d325-48f9-b765-829d4c799bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102885\n",
      "Counter({0: 57418, 2: 37347, 1: 8103, 3: 17})\n"
     ]
    }
   ],
   "source": [
    "print(len(partitions))\n",
    "from collections import Counter\n",
    "print(Counter(partitions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0ffc7-1646-4dfb-97c9-d1df27c7bda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_df.set_index(\"product_id\", inplace=True)\n",
    "\n",
    "product_df['cluster_id'] = partitions\n",
    "product_restrained_df = product_df[~product_df['cluster_id'].isnull()]\n",
    "product_restrained_df.astype({'cluster_id': int})\n",
    "product_restrained_df['name_descr'] = product_restrained_df.product_name +  \". \" + product_restrained_df.product_long_description + \". \"\n",
    "cluster_serie = product_restrained_df.groupby(\"cluster_id\").name_descr.sum()\n",
    "print(cluster_serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1f088d-545e-477f-acc9-0b6aba4ab22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(clusters)\n\u001b[0;32m----> 9\u001b[0m top_words_per_cluster_count \u001b[38;5;241m=\u001b[39m [get_top_tf_idf_words(item, \u001b[38;5;241m0.015\u001b[39m, \u001b[38;5;241m5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m vector]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m top_words \u001b[38;5;129;01min\u001b[39;00m top_words_per_cluster_count:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top_words)\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(clusters)\n\u001b[0;32m----> 9\u001b[0m top_words_per_cluster_count \u001b[38;5;241m=\u001b[39m [\u001b[43mget_top_tf_idf_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m vector]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m top_words \u001b[38;5;129;01min\u001b[39;00m top_words_per_cluster_count:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top_words)\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mget_top_tf_idf_words\u001b[0;34m(response, threshold, top_n)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(response_normalized.data)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sorted_nzs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(response_normalized\u001b[38;5;241m.\u001b[39mdata)[:\u001b[38;5;241m-\u001b[39m(top_n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_array\u001b[49m[response_normalized\u001b[38;5;241m.\u001b[39mindices[sorted_nzs[ response_normalized\u001b[38;5;241m.\u001b[39mindices[sorted_nzs] \u001b[38;5;241m>\u001b[39m threshold]]]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin, \u001b[38;5;241m0\u001b[39m, res)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_array' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "# COUNT VECTORIZER METHOD\n",
    "vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('french'))\n",
    "vector = vectorizer.fit_transform(clusters)\n",
    "top_words_per_cluster_count = [get_top_tf_idf_words(item, 0.015, 5) for item in vector]\n",
    "for top_words in top_words_per_cluster_count:\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181691ad-9c52-463e-946c-3c57566195f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Retrieve data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    CONCAT(\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    PRODUCT_LONG_DESCRIPTION;\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m secrets \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mload_secrets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries/secrets.yml\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowflake\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_snowflake_connection(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msecrets) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m     25\u001b[0m     co_viewed_df \u001b[38;5;241m=\u001b[39m query_co_viewed_df(con)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "from cdiscount.snowflake import get_snowflake_connection, query_snowflake_to_df\n",
    "# Retrieve data\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    CONCAT(\n",
    "        MAX(PRODUCT_PROPERTY_NAME),\n",
    "        '=',\n",
    "        PRODUCT_PROPERTY_VALUE\n",
    "    ) AS PRODUCT_PROPERTY_VALUE,\n",
    "    PRODUCT_ID,\n",
    "    PRODUCT_CATEGORY_LEVEL4_ID,\n",
    "    PRODUCT_CATEGORY_LEVEL4_NAME,\n",
    "    PRODUCT_LONG_DESCRIPTION\n",
    "FROM QUALISCORE_LAB.LAB_STAGE_VINCENT_CANAPES_WITH_PROPERTIES\n",
    "GROUP BY\n",
    "    PRODUCT_PROPERTY_ID,\n",
    "    PRODUCT_PROPERTY_VALUE,\n",
    "    PRODUCT_ID,\n",
    "    PRODUCT_CATEGORY_LEVEL4_ID,\n",
    "    PRODUCT_CATEGORY_LEVEL4_NAME,\n",
    "    PRODUCT_LONG_DESCRIPTION;\n",
    "\"\"\"\n",
    "secrets = config.load_secrets('queries/secrets.yml', key='snowflake')\n",
    "with get_snowflake_connection(**secrets) as con:\n",
    "    co_viewed_df = query_co_viewed_df(con)\n",
    "    product_df = query_canape_product_df(con)\n",
    "    edges = query_snowflake_to_df(sql_query_or_path=query, con=con)\n",
    "print('-' * 100)\n",
    "print(f'# of edges : {len(edges)}')\n",
    "print(f'# of product nodes : {edges.product_id.nunique()}')\n",
    "print(f'# of property nodes : {edges.product_property_value.nunique()}')\n",
    "print(edges.columns)\n",
    "print()\n",
    "\n",
    "# Create graph\n",
    "G = nx.Graph()\n",
    "colors = {'product_property_value': 'green', 'product_id': 'red'}\n",
    "for node_type, color in colors.items():\n",
    "    G.add_nodes_from(edges[node_type].unique(), color=[color] * edges[node_type].nunique())\n",
    "G.add_edges_from(edges[['product_property_value', 'product_id']].to_records(index=False).tolist())\n",
    "G.add_edges_from(co_viewed_df[['ref_id', 'related_id']].to_records(index=False).tolist())\n",
    "# Run community algorithm\n",
    "clustering = nx.community.louvain_communities(G, seed=42)\n",
    "\n",
    "# Format clustering results\n",
    "clusters = []\n",
    "for cluster_label, products in enumerate(clustering):\n",
    "    clusters.append(\n",
    "        pd.DataFrame({\n",
    "            'cluster': [f'cluster_{cluster_label}'] * len(products),\n",
    "            'product_id': list(products),\n",
    "        })\n",
    "    )\n",
    "clusters = pd.concat(clusters)\n",
    "print(clusters.columns)\n",
    "print('-' * 100)\n",
    "print(f'Number of clusters : {clusters.cluster.nunique()}')\n",
    "print()\n",
    "\n",
    "# Print cluster sizes\n",
    "print('-' * 100)\n",
    "print('Cluster sizes:')\n",
    "print(clusters.groupby('cluster').size().sort_values(ascending=False))\n",
    "print()\n",
    "\n",
    "# Count property nodes within the clusters\n",
    "n_property_nodes_by_cluster = (\n",
    "    clusters\n",
    "    .loc[lambda x: x.product_id.str.contains('=')]\n",
    "    .groupby('cluster').size()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print('-' * 100)\n",
    "print('Number of property nodes within the cluster :')\n",
    "print(n_property_nodes_by_cluster)\n",
    "print()\n",
    "\n",
    "# Assign product categories to cluster's product nodes\n",
    "product_categories = edges[['product_id', 'product_category_level4_id', 'product_category_level4_name', 'product_long_description']].drop_duplicates()\n",
    "clusters = clusters.merge(right=product_categories, how='inner', on='product_id')\n",
    "# print('-' * 100)\n",
    "# print('')\n",
    "# print(clusters.head(5))\n",
    "# print()\n",
    "\n",
    "# Print product categories present inside cluster 0\n",
    "print('-' * 100)\n",
    "print('Main categories inside cluster 0')\n",
    "print(clusters.groupby('cluster').product_category_level4_name.value_counts()['cluster_0'])\n",
    "print()\n",
    "\n",
    "# Find property nodes connected to each cluster (edges with starting node being a product node belonging to a given cluster)\n",
    "clusters_with_properties = clusters.merge(right=edges[['product_id', 'product_property_value']], how='inner', on='product_id')\n",
    "# print(clusters_with_properties.head(5))\n",
    "\n",
    "\n",
    "# Print top 3 property nodes connected to cluster 0 (Property \"nature\" of the cluster)\n",
    "print('-' * 100)\n",
    "print('Main properties connected to cluster 0')\n",
    "print(clusters_with_properties.groupby('cluster').product_property_value.value_counts()['cluster_0'].head(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "196e3b2d-be2b-4e92-9346-4ac495042690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_long_description</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>cluster</th>\n",
       "      <th>product_category_level4_id</th>\n",
       "      <th>product_category_level4_name</th>\n",
       "      <th>product_property_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC3094814788770</th>\n",
       "      <td>Couvre pour Canapé,Housses de canapé avec Jupe...</td>\n",
       "      <td>Couvre pour Canapé,Housses de canapé avec Jupe...</td>\n",
       "      <td>None</td>\n",
       "      <td>HOUSSE DE CANAPE</td>\n",
       "      <td>cluster_11</td>\n",
       "      <td>1000001706</td>\n",
       "      <td>HOUSSE DE CANAPE</td>\n",
       "      <td>Couleur principale=Bleu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC1694790220911</th>\n",
       "      <td>Housse De Canapé Dangle Matelassé,4-3-2-1 Plac...</td>\n",
       "      <td>Housse de Canapé Dangle Matelassé : La housse ...</td>\n",
       "      <td>None</td>\n",
       "      <td>HOUSSE DE CANAPE</td>\n",
       "      <td>cluster_2</td>\n",
       "      <td>1000001706</td>\n",
       "      <td>HOUSSE DE CANAPE</td>\n",
       "      <td>Couleur principale=Jaune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC3028235866830</th>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>None</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>cluster_4</td>\n",
       "      <td>1000002639</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>Matière du revêtement=Tissu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC3028235866830</th>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>None</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>cluster_4</td>\n",
       "      <td>1000002639</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>Type de canapé=Fixe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC3028235866830</th>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>ZHG- MODE&amp;CHIC 3056621 2 Piece Sofa Set Fabric...</td>\n",
       "      <td>None</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>cluster_4</td>\n",
       "      <td>1000002639</td>\n",
       "      <td>CANAPE - SOFA - DIVAN</td>\n",
       "      <td>Couleur principale=Rouge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       product_name  \\\n",
       "product_id                                                            \n",
       "AUC3094814788770  Couvre pour Canapé,Housses de canapé avec Jupe...   \n",
       "AUC1694790220911  Housse De Canapé Dangle Matelassé,4-3-2-1 Plac...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "\n",
       "                                           product_long_description  \\\n",
       "product_id                                                            \n",
       "AUC3094814788770  Couvre pour Canapé,Housses de canapé avec Jupe...   \n",
       "AUC1694790220911  Housse de Canapé Dangle Matelassé : La housse ...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "AUC3028235866830  ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...   \n",
       "\n",
       "                 brand_name  product_category_name     cluster  \\\n",
       "product_id                                                       \n",
       "AUC3094814788770       None       HOUSSE DE CANAPE  cluster_11   \n",
       "AUC1694790220911       None       HOUSSE DE CANAPE   cluster_2   \n",
       "AUC3028235866830       None  CANAPE - SOFA - DIVAN   cluster_4   \n",
       "AUC3028235866830       None  CANAPE - SOFA - DIVAN   cluster_4   \n",
       "AUC3028235866830       None  CANAPE - SOFA - DIVAN   cluster_4   \n",
       "\n",
       "                  product_category_level4_id product_category_level4_name  \\\n",
       "product_id                                                                  \n",
       "AUC3094814788770                  1000001706             HOUSSE DE CANAPE   \n",
       "AUC1694790220911                  1000001706             HOUSSE DE CANAPE   \n",
       "AUC3028235866830                  1000002639        CANAPE - SOFA - DIVAN   \n",
       "AUC3028235866830                  1000002639        CANAPE - SOFA - DIVAN   \n",
       "AUC3028235866830                  1000002639        CANAPE - SOFA - DIVAN   \n",
       "\n",
       "                       product_property_value  \n",
       "product_id                                     \n",
       "AUC3094814788770      Couleur principale=Bleu  \n",
       "AUC1694790220911     Couleur principale=Jaune  \n",
       "AUC3028235866830  Matière du revêtement=Tissu  \n",
       "AUC3028235866830          Type de canapé=Fixe  \n",
       "AUC3028235866830     Couleur principale=Rouge  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_restrained_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d888510-8e89-48c9-a951-f5ade12eceea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_product_df = product_df.merge(clusters_with_properties, on=['product_id', 'product_long_description'], how='inner').drop_duplicates(subset=['product_id'])\n",
    "merged_product_df.set_index(\"product_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0530df5c-b170-4c44-beb4-f7c8fc5881c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_restrained_df = merged_product_df[~merged_product_df['cluster'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0597cd7-8727-4ab4-b3f8-869553e6e24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_restrained_df['name_descr'] = product_restrained_df.product_name +  \". \" + product_restrained_df.product_long_description + \". \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a9ba08-f74e-4f7a-b61d-c5d8703e22de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_restrained_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35ffa53-7c04-4ce3-a1d8-8d715a22c4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_serie = product_restrained_df.groupby(\"cluster\").name_descr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc60de3-ee8c-4b50-88c9-361f4ec0abb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "cluster_0     Housse de canapé élastique Housse de canapé él...\n",
       "cluster_1     Ensemble de canapés 2pcs 72x78x74cm - 1 fauteu...\n",
       "cluster_10    Tbest housse de canapé New Hot 7 Solide Pure C...\n",
       "cluster_11    Couvre pour Canapé,Housses de canapé avec Jupe...\n",
       "cluster_12    JIM-7329026656651-Couch Sofa Cover, Cushion Co...\n",
       "cluster_13    Atyhao Canapé pour enfants Gris clair Peluche ...\n",
       "cluster_2     Housse De Canapé Dangle Matelassé,4-3-2-1 Plac...\n",
       "cluster_3     Meilleurs Meubles Canapé-lit réglable avec 2 o...\n",
       "cluster_4     ZHG- MODE&CHIC 3056621 2 Piece Sofa Set Fabric...\n",
       "cluster_5     Canapé 3 places convertible clic-clac en tissu...\n",
       "cluster_6     Housse de canapé Extensible Décoration du Mais...\n",
       "cluster_7     Canapé d'angle RIO Convertible avec coffre en ...\n",
       "cluster_8     Ensemble de canapés 2 pcs avec coussins Rotin ...\n",
       "cluster_9     ESTINK Canapé pour enfants à 2 places Crème Pe...\n",
       "Name: name_descr, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ec734e-d99e-4649-871f-211d44845fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import check_array\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "class ClassTfidfTransformer(TfidfTransformer):\n",
    "    \"\"\"\n",
    "    A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base.\n",
    "\n",
    "    ![](../algorithm/c-TF-IDF.svg)\n",
    "\n",
    "    c-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes\n",
    "    by joining all documents per class. Thus, each class is converted to a single document\n",
    "    instead of set of documents. The frequency of each word **x** is extracted\n",
    "    for each class **c** and is **l1** normalized. This constitutes the term frequency.\n",
    "\n",
    "    Then, the term frequency is multiplied with IDF which is the logarithm of 1 plus\n",
    "    the average number of words per class **A** divided by the frequency of word **x**\n",
    "    across all classes.\n",
    "\n",
    "    Arguments:\n",
    "        bm25_weighting: Uses BM25-inspired idf-weighting procedure instead of the procedure\n",
    "                        as defined in the c-TF-IDF formula. It uses the following weighting scheme:\n",
    "                        `log(1+((avg_nr_samples - df + 0.5) / (df+0.5)))`\n",
    "        reduce_frequent_words: Takes the square root of the bag-of-words after normalizing the matrix.\n",
    "                               Helps to reduce the impact of words that appear too frequently.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    ```python\n",
    "    transformer = ClassTfidfTransformer()\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, bm25_weighting: bool = False, reduce_frequent_words: bool = False):\n",
    "        self.bm25_weighting = bm25_weighting\n",
    "        self.reduce_frequent_words = reduce_frequent_words\n",
    "        super(ClassTfidfTransformer, self).__init__()\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, multiplier: np.ndarray = None):\n",
    "        \"\"\"Learn the idf vector (global term weights).\n",
    "\n",
    "        Arguments:\n",
    "            X: A matrix of term/token counts.\n",
    "            multiplier: A multiplier for increasing/decreasing certain IDF scores\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=('csr', 'csc'))\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csr_matrix(X)\n",
    "        dtype = np.float64\n",
    "\n",
    "        if self.use_idf:\n",
    "            _, n_features = X.shape\n",
    "\n",
    "            # Calculate the frequency of words across all classes\n",
    "            df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "\n",
    "            # Calculate the average number of samples as regularization\n",
    "            avg_nr_samples = int(X.sum(axis=1).mean())\n",
    "\n",
    "            # BM25-inspired weighting procedure\n",
    "            if self.bm25_weighting:\n",
    "                idf = np.log(1+((avg_nr_samples - df + 0.5) / (df+0.5)))\n",
    "\n",
    "            # Divide the average number of samples by the word frequency\n",
    "            # +1 is added to force values to be positive\n",
    "            else:\n",
    "                idf = np.log((avg_nr_samples / df)+1)\n",
    "\n",
    "            # Multiplier to increase/decrease certain idf scores\n",
    "            if multiplier is not None:\n",
    "                idf = idf * multiplier\n",
    "\n",
    "            self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                      shape=(n_features, n_features),\n",
    "                                      format='csr',\n",
    "                                      dtype=dtype)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix):\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF\n",
    "\n",
    "        Arguments:\n",
    "            X (sparse matrix): A matrix of term/token counts.\n",
    "\n",
    "        Returns:\n",
    "            X (sparse matrix): A c-TF-IDF matrix\n",
    "        \"\"\"\n",
    "        if self.use_idf:\n",
    "            X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "            if self.reduce_frequent_words:\n",
    "                X.data = np.sqrt(X.data)\n",
    "\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "class CustomClassTfidfTransformer(ClassTfidfTransformer):\n",
    "    def __init__(self, use_idf: bool = False, bm25_weighting: bool = False, reduce_frequent_words: bool = False):\n",
    "        super(CustomClassTfidfTransformer, self).__init__(\n",
    "            bm25_weighting=bm25_weighting,\n",
    "            reduce_frequent_words=reduce_frequent_words\n",
    "        )\n",
    "        self.use_idf = use_idf\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "        if self.use_idf:\n",
    "\n",
    "            if self.reduce_frequent_words:\n",
    "                X.data = np.sqrt(X.data)\n",
    "\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6c32574-9d3d-4151-b264-b48b4a6ad988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_tf_idf_words_2(feature_array, response, threshold, top_n=2, print_associated_threshold=False):\n",
    "    response_normalized = custom_norm(response)\n",
    "    response_normalized.data[response_normalized.data < 0] = 0.0 # TODO: Replace 0 with threshold\n",
    "    response_normalized.eliminate_zeros()\n",
    "    \n",
    "    sorted_nzs = np.argsort(response_normalized.data)[:-(top_n+1):-1]\n",
    "    \n",
    "    keywords = feature_array[response_normalized.indices[sorted_nzs[response_normalized.indices[sorted_nzs] > 0]]] # TODO: Replace 0 with threshold\n",
    "    if not print_associated_threshold:\n",
    "        return keywords\n",
    "\n",
    "    tfidf_scores = response_normalized.data[sorted_nzs]\n",
    "    res = list(zip(keywords, tfidf_scores))\n",
    "    return res\n",
    "\n",
    "def custom_norm(x):\n",
    "    norm = x.sum(axis=1)\n",
    "    return x / norm\n",
    "\n",
    "\n",
    "# Vector is the string to apply tf-idf on.\n",
    "def apply_tf_idf_2(vector, vectorizer, threshold, top_n=2, print_associated_threshold=False):\n",
    "    v = CustomClassTfidfTransformer(use_idf=True)\n",
    "    x = v.fit_transform(vector)\n",
    "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
    "    tfidf_sorting = np.argsort(x.toarray()).flatten()[::-1]\n",
    "    top_words_per_cluster_tfidf = [get_top_tf_idf_words_2(feature_array, item, threshold, top_n, print_associated_threshold) for item in x]\n",
    "    return top_words_per_cluster_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69afabfc-e987-4709-b2c5-1dfb405efb36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67f41aba-2f33-4e73-bbbe-b8934d6dbc77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Icarusight\n",
      "[('chrome', 0.012115407431163315), ('bouteille', 0.007353176579070996), ('115x60x67', 0.004530119928015576), ('émeraude', 0.003910951311076582), ('verte', 0.0039107190586954105)]\n",
      "[('queen', 0.005982745590004781), ('personnalisable', 0.005819566959259014), ('polychlorure', 0.0037035824156580804), ('vinyle', 0.003701746269223654), ('positionnez', 0.0034098629792197236)]\n",
      "[('monoplaces', 0.0157093318974846), ('complétez', 0.007154690428549345), ('a341796', 0.005625706951867143), ('sécuritaire', 0.00538946697069688), ('a341844', 0.004559713715820153)]\n",
      "[('indigo', 0.0070768360130667755), ('chrome', 0.00478876175380354), ('collant', 0.0040036873904282155), ('illustrer', 0.0040036873904282155), ('vendues', 0.004000481418860051)]\n",
      "[('résine', 0.007195334958122552), ('tressée', 0.007144034989381584), ('rotin', 0.005134699290375354), ('mengyyshop', 0.004271016315226769), ('jersey', 0.004028496464307161)]\n",
      "[('étoiles', 0.008095981968049091), ('uyeoco', 0.006114437336541146), ('collecter', 0.0048451591219541715), ('siliconé', 0.0048451591219541715), ('sdnam', 0.003800584716585823)]\n",
      "[('rotin', 0.011081526728376846), ('résine', 0.007337817817425579), ('tressée', 0.007321202960080139), ('aluminium', 0.005524239038931699), ('manguier', 0.005331160029712072)]\n",
      "[('endommager', 0.013502755064564035), ('maintes', 0.006396829153770217), ('cen', 0.006334034508216926), ('polychlorure', 0.005609374038596972), ('vinyle', 0.005606593046847799)]\n",
      "[('specification', 0.007095912761289471), ('amour', 0.007066168000486147), ('facultatif', 0.006867914370596013), ('extrême', 0.004963181344512229), ('présentés', 0.0049494690462394434)]\n",
      "[('rotin', 0.008064204791690593), ('manguier', 0.0051504764007136135), ('dreamer', 0.0036206213664114824), ('sun', 0.003294250224792712), ('miliboo', 0.0031804414014935107)]\n",
      "[('lilas', 0.0216478499417636), ('mauve', 0.02092586254850048), ('parme', 0.01287056630375516), ('lavande', 0.011876425351528833), ('lila', 0.008541969032792494)]\n",
      "[('orange', 0.007109144622321689), ('techniquenombre', 0.006614525451962082), ('europeposition', 0.00595608841647223), ('gt', 0.005850981167919374), ('italien', 0.0057616660597116735)]\n",
      "[('rotin', 0.02196641377852014), ('manguier', 0.009990544514232666), ('résine', 0.005095408015175097), ('tressée', 0.005082387176704454), ('plcs', 0.005045391797504083)]\n",
      "[('sdnam', 0.005724304928619626), ('collecter', 0.00359436033654509), ('siliconé', 0.00359436033654509), ('envers', 0.0030930005486378993), ('jersey', 0.0030717770247817512)]\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "from Icarusight.Icarusight.vectorizer.comunities_naming import apply_tf_idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('french'), min_df=0.05, max_df=0.7)\n",
    "vector = vectorizer.fit_transform(cluster_serie)\n",
    "dominant_words = apply_tf_idf_2(vector, vectorizer, threshold=0.001, top_n=5, print_associated_threshold=True)\n",
    "for dominant_word in dominant_words:\n",
    "    print(dominant_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f282f99-8575-450d-b9e6-ca0f0078ce37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Theme Key Phrase: cm, totales, canapé, revêtement, cadre\n",
      "cluster_0 cm, totales, canapé, revêtement, cadre\n",
      "Main Theme Key Phrase: siège, cm, partir, sol, totales\n",
      "cluster_1 siège, cm, partir, sol, totales\n",
      "Main Theme Key Phrase: housse, canapé, extensible, taille, places\n",
      "cluster_2 housse, canapé, extensible, taille, places\n",
      "Main Theme Key Phrase: coussin, cm, canapé, dimensions, siège\n",
      "cluster_3 coussin, cm, canapé, dimensions, siège\n",
      "Main Theme Key Phrase: canapé, housse, places, coussin, tissu\n",
      "cluster_4 canapé, housse, places, coussin, tissu\n",
      "Main Theme Key Phrase: siège, partir, sol, cm, cadre\n",
      "cluster_5 siège, partir, sol, cm, cadre\n",
      "Main Theme Key Phrase: siège, coussin, partir, sol, cm\n",
      "cluster_6 siège, coussin, partir, sol, cm\n",
      "Main Theme Key Phrase: cm, canapé, mousse, tissu, dimensions\n",
      "cluster_7 cm, canapé, mousse, tissu, dimensions\n",
      "Main Theme Key Phrase: facile, revêtement, cadre, totales, pieds\n",
      "cluster_8 facile, revêtement, cadre, totales, pieds\n",
      "Main Theme Key Phrase: siège, cm, coussin, partir, sol\n",
      "cluster_9 siège, cm, coussin, partir, sol\n",
      "Main Theme Key Phrase: canapé, housse, places, tissu, housses\n",
      "cluster_10 canapé, housse, places, tissu, housses\n",
      "Main Theme Key Phrase: siège, cm, dossier, sol, remplissage\n",
      "cluster_11 siège, cm, dossier, sol, remplissage\n",
      "Main Theme Key Phrase: housse, canapé, extensible, places, tissu\n",
      "cluster_12 housse, canapé, extensible, places, tissu\n"
     ]
    }
   ],
   "source": [
    "from Icarusight.Icarusight.flatXcoviewed_model.comunities_naming import find_dominant_words, apply_tf_idf\n",
    "\n",
    "cluster_list = clusters[\"cluster\"].drop_duplicates().tolist()\n",
    "\n",
    "for cluster_id in cluster_list:\n",
    "    cluster_serie = clusters.loc[clusters['cluster'] == cluster_id, \"product_long_description\"]\n",
    "    descriptions = cluster_serie.tolist()\n",
    "    dominant_words = find_dominant_words(descriptions)\n",
    "    print(cluster_id, dominant_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (icarusight)",
   "language": "python",
   "name": "icarusight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
